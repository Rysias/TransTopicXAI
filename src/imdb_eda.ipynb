{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB EDA\n",
    "Initial exploration of the imdb data with the following goals: \n",
    "- What columns exist in the data? \n",
    "- How much? \n",
    "- How long are the documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Union, List\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import fileinput as fi\n",
    "\n",
    "DATA_DIR = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_text_dat(dr: Path) -> List[Path]:\n",
    "    return list(dr.glob(\"*.txt\"))\n",
    "\n",
    "def glob_all_reviews(dr: Path) -> List[Path]:\n",
    "    return glob_text_dat(dr / \"pos\") + glob_text_dat(dr / \"neg\")\n",
    "\n",
    "def read_review(review_path: Path) -> str:\n",
    "    with open(review_path, \"r\", encoding=\"utf8\") as f:\n",
    "        return f.readline()\n",
    "\n",
    "def review_to_row(line: str, review_path: Path) -> Dict[str, Union[str, int]]:\n",
    "    review_id = review_path.name[:-4]\n",
    "    label = 1 if review_path.parent.name == \"pos\" else 0\n",
    "    origin = review_path.parent.parent.name\n",
    "    return {\"id\": review_id, \"origin\": origin, \"text\": line, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping the data files :)\n",
    "data_path = next(DATA_DIR.glob(\"*tar.gz\"))\n",
    "\n",
    "if not (DATA_DIR / \"aclimdb\").exists():\n",
    "    with tarfile.open(data_path, 'r:gz') as tar:\n",
    "        tar.extractall(path = DATA_DIR)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create useful training data \n",
    "Now that they are unzipped, I will put them into a pandas dataframe which is easier to load :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAIN_DIR = DATA_DIR / \"aclimdb\"\n",
    "\n",
    "test_paths = glob_all_reviews(MAIN_DIR / \"test\")\n",
    "train_paths = glob_all_reviews(MAIN_DIR / \"train\")\n",
    "all_paths = test_paths + train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing 1000 files\n",
      "done processing 2000 files\n",
      "done processing 3000 files\n",
      "done processing 4000 files\n",
      "done processing 5000 files\n",
      "done processing 6000 files\n",
      "done processing 7000 files\n",
      "done processing 8000 files\n",
      "done processing 9000 files\n",
      "done processing 10000 files\n",
      "done processing 11000 files\n",
      "done processing 12000 files\n",
      "done processing 13000 files\n",
      "done processing 14000 files\n",
      "done processing 15000 files\n",
      "done processing 16000 files\n",
      "done processing 17000 files\n",
      "done processing 18000 files\n",
      "done processing 19000 files\n",
      "done processing 20000 files\n",
      "done processing 21000 files\n",
      "done processing 22000 files\n",
      "done processing 23000 files\n",
      "done processing 24000 files\n",
      "done processing 25000 files\n",
      "done processing 26000 files\n",
      "done processing 27000 files\n",
      "done processing 28000 files\n",
      "done processing 29000 files\n",
      "done processing 30000 files\n",
      "done processing 31000 files\n",
      "done processing 32000 files\n",
      "done processing 33000 files\n",
      "done processing 34000 files\n",
      "done processing 35000 files\n",
      "done processing 36000 files\n",
      "done processing 37000 files\n",
      "done processing 38000 files\n",
      "done processing 39000 files\n",
      "done processing 40000 files\n",
      "done processing 41000 files\n",
      "done processing 42000 files\n",
      "done processing 43000 files\n",
      "done processing 44000 files\n",
      "done processing 45000 files\n",
      "done processing 46000 files\n",
      "done processing 47000 files\n",
      "done processing 48000 files\n",
      "done processing 49000 files\n",
      "done processing 50000 files\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "review_list = [dict.fromkeys([\"id\", \"origin\", \"text\", \"label\"]) for _ in range(len(all_paths))]\n",
    "with fi.input(all_paths, openhook=fi.hook_encoded(\"utf-8\")) as f:\n",
    "    for line in f:\n",
    "        review_list[i] = review_to_row(line, f.filename())\n",
    "        i += 1\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"done processing {i} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame.from_records(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f86e4359b2b5cf5d398b363c816bb50d8f4fd2c9e9291492e7c9219700f63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('bertopic_explore': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
