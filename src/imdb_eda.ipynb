{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB EDA\n",
    "Initial exploration of the imdb data with the following goals: \n",
    "- What columns exist in the data? \n",
    "- How much? \n",
    "- How long are the documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Union, List\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import fileinput as fi\n",
    "\n",
    "DATA_DIR = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_text_dat(dr: Path) -> List[Path]:\n",
    "    return list(dr.glob(\"*.txt\"))\n",
    "\n",
    "def glob_all_reviews(dr: Path) -> List[Path]:\n",
    "    return glob_text_dat(dr / \"pos\") + glob_text_dat(dr / \"neg\")\n",
    "\n",
    "def read_review(review_path: Path) -> str:\n",
    "    with open(review_path, \"r\", encoding=\"utf8\") as f:\n",
    "        return f.readline()\n",
    "\n",
    "def review_to_row(line: str, review_path: Path) -> Dict[str, Union[str, int]]:\n",
    "    review_id = review_path.name[:-4]\n",
    "    label = 1 if review_path.parent.name == \"pos\" else 0\n",
    "    origin = review_path.parent.parent.name\n",
    "    return {\"id\": review_id, \"origin\": origin, \"text\": line, \"label\": label}\n",
    "\n",
    "    \n",
    "def get_movie_id(movie_url: str) -> str:\n",
    "    return re.search(\"tt\\d+\", movie_url).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping the data files :)\n",
    "data_path = next(DATA_DIR.glob(\"*tar.gz\"))\n",
    "\n",
    "if not (DATA_DIR / \"aclimdb\").exists():\n",
    "    with tarfile.open(data_path, 'r:gz') as tar:\n",
    "        tar.extractall(path = DATA_DIR)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create useful training data \n",
    "Now that they are unzipped, I will put them into a pandas dataframe which is easier to load :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAIN_DIR = DATA_DIR / \"aclimdb\"\n",
    "\n",
    "test_paths = glob_all_reviews(MAIN_DIR / \"test\")\n",
    "train_paths = glob_all_reviews(MAIN_DIR / \"train\")\n",
    "all_paths = test_paths + train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing 5000 files\n",
      "done processing 10000 files\n",
      "done processing 15000 files\n",
      "done processing 20000 files\n",
      "done processing 25000 files\n",
      "done processing 30000 files\n",
      "done processing 35000 files\n",
      "done processing 40000 files\n",
      "done processing 45000 files\n",
      "done processing 50000 files\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "review_list = [dict.fromkeys([\"id\", \"origin\", \"text\", \"label\"]) for _ in range(len(all_paths))]\n",
    "with fi.input(all_paths, openhook=fi.hook_encoded(\"utf-8\")) as f:\n",
    "    for line in f:\n",
    "        review_list[i] = review_to_row(line, f.filename())\n",
    "        i += 1\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"done processing {i} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame.from_records(review_list)\n",
    "all_data.to_csv(DATA_DIR / \"all_data.csv\")\n",
    "all_data = pd.read_csv(DATA_DIR / \"all_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train test\n",
    "Now I split into test set making sure not to have leakage for movies :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = pd.DataFrame(columns = [\"movie_id\", \"source\", \"linenum\"], index = range(len(test_paths)))\n",
    "with fi.input((MAIN_DIR / \"test\").glob(\"*.txt\"), openhook=fi.hook_encoded(\"utf-8\")) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        all_movies.loc[i, \"movie_id\"] = line\n",
    "        all_movies.loc[i, \"linenum\"] = f.filelineno() - 1\n",
    "        all_movies.loc[i, \"source\"] = f.filename().name\n",
    "all_movies[\"movie_id\"] = all_movies[\"movie_id\"].str.extract(\"tt(\\d+)\").values\n",
    "all_movies[\"source\"] = all_movies[\"source\"].str.contains(\"pos\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose N hundred movies\n",
    "N = 700\n",
    "unique_movies = all_movies[\"movie_id\"].unique()\n",
    "np.random.seed(42)\n",
    "target_movies = np.random.choice(unique_movies, N)\n",
    "\n",
    "# Create ids\n",
    "test_movies = all_movies[all_movies[\"movie_id\"].isin(target_movies)]\n",
    "test_ids = test_movies[\"source\"].astype(str) + \"_\" + test_movies[\"linenum\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"test_id\"] = all_data[\"label\"].astype(str) + \"_\" + pd.Series(all_data[\"id\"].str.extract(\"(\\d+)_\\d+\").values.reshape((-1, )))\n",
    "all_data.loc[all_data[\"origin\"] == \"train\", \"test_id\"]  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = all_data.loc[all_data[\"test_id\"].isin(test_ids), [\"text\", \"label\"]].reset_index(drop=True)\n",
    "train_data = all_data.loc[~all_data[\"test_id\"].isin(test_ids), [\"text\", \"label\"]].reset_index(drop=True)\n",
    "test_data.to_csv(DATA_DIR / \"imdb_test.csv\")\n",
    "train_data.to_csv(DATA_DIR / \"imdb_train.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f86e4359b2b5cf5d398b363c816bb50d8f4fd2c9e9291492e7c9219700f63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('bertopic_explore': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
