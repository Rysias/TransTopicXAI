{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantitative Evaluations of Topic Models\r\n",
    "This notebook is for evaluating the quality of the topic models. Mainly it will be a running benchmark of my models + a comparison to LDA (when I set that up)\r\n",
    "\r\n",
    "### Learnings:\r\n",
    "- It is a bit of a hassle to convert sklearn / bertopic \r\n",
    "- remember to check for congruency between corpus and topics! (re-run pipeline)\r\n",
    "- "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "import pickle\r\n",
    "from bertopic import BERTopic\r\n",
    "from pathlib import Path\r\n",
    "from gensim.models.coherencemodel import CoherenceModel\r\n",
    "from gensim.matutils import Sparse2Corpus\r\n",
    "from gensim.corpora import Dictionary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "def read_pickle(file_path):\r\n",
    "    with open(file_path, \"rb\") as f:\r\n",
    "        return pickle.load(f)\r\n",
    "\r\n",
    "def pickle_object(obj, file_path):\r\n",
    "    with open(file_path, \"wb\") as f:\r\n",
    "        pickle.dump(obj, f)\r\n",
    "\r\n",
    "def flatten_list(lst):\r\n",
    "    return [elem for sublist in lst for elem in sublist]\r\n",
    "    \r\n",
    "def get_paragraphs(paragraph_dict):\r\n",
    "    return flatten_list(list(paragraph_dict.values()))\r\n",
    "    \r\n",
    "def is_lower(s):\r\n",
    "    return s.lower() == s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# Loading topic model\r\n",
    "MODEL_PATH = Path(\"../models/\")\r\n",
    "DATA_DIR = Path(\"../../BscThesisData/data\")\r\n",
    "topic_model = BERTopic.load(str(MODEL_PATH / \"topic_model\"), embedding_model=\"Maltehb/-l-ctra-danish-electra-small-cased\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\jhr/.cache\\torch\\sentence_transformers\\Maltehb_-l-ctra-danish-electra-small-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\jhr/.cache\\torch\\sentence_transformers\\Maltehb_-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.bias', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.10.attention.self.query.weight', 'generator_predictions.dense.bias', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.embeddings.word_embeddings.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.11.intermediate.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.value.bias', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.embeddings_project.weight', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.5.output.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.embeddings_project.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.embeddings.position_embeddings.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.key.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator_predictions.dense.weight', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.1.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.value.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.4.attention.self.query.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.1.attention.output.dense.weight', 'generator.encoder.layer.0.attention.self.query.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "clean_paragraphs = read_pickle(DATA_DIR / \"paragraph_dict.pkl\")\r\n",
    "paragraph_list = get_paragraphs(clean_paragraphs)\r\n",
    "vectorizer = read_pickle(MODEL_PATH / \"vectorizer.pkl\")\r\n",
    "X = vectorizer.fit_transform(paragraph_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# transform sparse matrix into gensim corpus\r\n",
    "corpus_vect_gensim = Sparse2Corpus(X, documents_columns=False)\r\n",
    "dictionary = Dictionary.from_corpus(corpus_vect_gensim,\r\n",
    "                                    id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\r\n",
    "tokenized_docs = [[word.lower() for word in tokenizer(doc) if not word.lower() in vectorizer.stop_words] for doc in paragraph_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "# Getting the Topics \r\n",
    "topics = topic_model.get_topics()\r\n",
    "topic_list = [[item[0] for item in topic] for key, topic in topics.items() if not key==-1]\r\n",
    "assert len(topic_list) == len(list(topics.keys())) - 1\r\n",
    "assert type(topic_list) == list\r\n",
    "assert type(topic_list[0]) == list\r\n",
    "assert type(topic_list[0][0]) == str"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "cm = CoherenceModel(topics=topic_list, coherence=\"c_v\", dictionary=dictionary, texts=tokenized_docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "cm.get_coherence_per_topic()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5138103769278116,\n",
       " 0.6713135994442715,\n",
       " 0.6199477050234451,\n",
       " 0.4090339291266286,\n",
       " 0.47907132277369946]"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "all_words = set(word.lower() for doc in tokenized_docs for word in doc)\r\n",
    "topic_words = set(topic_list[0])\r\n",
    "\r\n",
    "for topic in topic_list:\r\n",
    "    topic_words = set(topic)\r\n",
    "    print(f\"superflous words: {topic_words - all_words}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "# Test if it works to remove invalid words :)) \r\n",
    "clean_topic_list = [[word for word in topic if word in all_words] for topic in topic_list]\r\n",
    "cm = CoherenceModel(topics=clean_topic_list, coherence=\"c_v\", dictionary=dictionary, texts=tokenized_docs)\r\n",
    "print(cm.get_coherence_per_topic())\r\n",
    "print(cm.get_coherence())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.3043144840452862, 0.27015012488366497, 0.4865898101777429, 0.6728787468629828, 0.41105431905355916, 0.416226571950254]\n",
      "0.4268690094955816\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It does!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('bertopic_explore': conda)"
  },
  "interpreter": {
   "hash": "d6f86e4359b2b5cf5d398b363c816bb50d8f4fd2c9e9291492e7c9219700f63c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}