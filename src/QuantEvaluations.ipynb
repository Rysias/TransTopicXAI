{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Evaluations of Topic Models\n",
    "This notebook is for evaluating the quality of the topic models. Mainly it will be a running benchmark of my models + a comparison to LDA (when I set that up)\n",
    "\n",
    "### Learnings:\n",
    "- It is a bit of a hassle to convert sklearn / bertopic \n",
    "- remember to check for congruency between corpus and topics! (re-run pipeline)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "from pathlib import Path\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def pickle_object(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def flatten_list(lst):\n",
    "    return [elem for sublist in lst for elem in sublist]\n",
    "    \n",
    "def get_paragraphs(paragraph_dict):\n",
    "    return flatten_list(list(paragraph_dict.values()))\n",
    "    \n",
    "def is_lower(s):\n",
    "    return s.lower() == s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\jhr/.cache\\torch\\sentence_transformers\\Maltehb_-l-ctra-danish-electra-small-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\jhr/.cache\\torch\\sentence_transformers\\Maltehb_-l-ctra-danish-electra-small-cased were not used when initializing ElectraModel: ['generator.encoder.layer.7.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.value.weight', 'generator.encoder.layer.7.attention.output.dense.bias', 'generator.encoder.layer.9.attention.self.key.bias', 'generator.encoder.layer.1.intermediate.dense.bias', 'generator.encoder.layer.5.attention.self.query.weight', 'generator.encoder.layer.3.output.dense.bias', 'generator.encoder.layer.10.attention.output.dense.bias', 'generator.encoder.layer.9.intermediate.dense.bias', 'generator.encoder.layer.2.output.LayerNorm.bias', 'generator.encoder.layer.4.attention.self.value.bias', 'generator.encoder.layer.9.attention.self.query.bias', 'generator.encoder.layer.2.attention.self.key.bias', 'discriminator_predictions.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.weight', 'generator.encoder.layer.10.attention.self.key.bias', 'generator.encoder.layer.9.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.output.dense.bias', 'generator.encoder.layer.4.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.dense.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.weight', 'generator.embeddings.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.value.weight', 'generator.encoder.layer.2.intermediate.dense.bias', 'generator.encoder.layer.11.attention.self.query.bias', 'generator.encoder.layer.1.attention.output.dense.bias', 'generator.encoder.layer.8.attention.output.dense.bias', 'generator.encoder.layer.10.attention.self.query.weight', 'generator_predictions.dense.bias', 'discriminator_predictions.classifier.bias', 'generator.encoder.layer.5.attention.output.dense.weight', 'generator.encoder.layer.9.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.encoder.layer.8.intermediate.dense.weight', 'generator.encoder.layer.4.output.dense.weight', 'generator.encoder.layer.4.attention.self.query.bias', 'generator.encoder.layer.8.attention.self.key.weight', 'generator.embeddings.word_embeddings.weight', 'generator.encoder.layer.10.attention.output.dense.weight', 'generator.encoder.layer.8.attention.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.self.key.weight', 'generator.encoder.layer.0.intermediate.dense.bias', 'generator.encoder.layer.2.attention.self.key.weight', 'generator.encoder.layer.11.attention.self.value.weight', 'generator.encoder.layer.11.output.LayerNorm.weight', 'generator.encoder.layer.3.attention.self.key.bias', 'generator.encoder.layer.3.output.LayerNorm.weight', 'generator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator_predictions.bias', 'generator.encoder.layer.6.intermediate.dense.bias', 'generator.encoder.layer.7.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.query.weight', 'generator.encoder.layer.3.attention.self.value.bias', 'generator.encoder.layer.6.output.LayerNorm.weight', 'generator.encoder.layer.0.intermediate.dense.weight', 'generator.encoder.layer.11.intermediate.dense.weight', 'discriminator_predictions.dense.bias', 'generator.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.encoder.layer.10.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.key.weight', 'generator.encoder.layer.5.output.dense.weight', 'generator.encoder.layer.6.output.dense.weight', 'generator.encoder.layer.11.attention.self.key.weight', 'generator.encoder.layer.3.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.output.dense.bias', 'generator.encoder.layer.6.attention.self.value.bias', 'generator.encoder.layer.10.attention.self.key.weight', 'generator.encoder.layer.1.attention.self.key.bias', 'generator.encoder.layer.9.intermediate.dense.weight', 'generator.embeddings_project.weight', 'generator.encoder.layer.9.output.dense.weight', 'generator.encoder.layer.11.attention.output.dense.weight', 'generator.encoder.layer.2.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.output.LayerNorm.weight', 'generator.encoder.layer.1.attention.self.value.weight', 'generator.encoder.layer.7.output.LayerNorm.weight', 'generator.encoder.layer.7.output.dense.weight', 'generator.encoder.layer.8.attention.self.value.bias', 'generator.encoder.layer.7.output.dense.bias', 'generator.encoder.layer.3.attention.self.value.weight', 'generator.encoder.layer.9.attention.output.dense.bias', 'generator.encoder.layer.4.output.LayerNorm.bias', 'generator.encoder.layer.10.intermediate.dense.bias', 'generator_predictions.LayerNorm.weight', 'generator.encoder.layer.5.output.dense.bias', 'generator.encoder.layer.10.output.LayerNorm.bias', 'generator.encoder.layer.9.output.dense.bias', 'generator.encoder.layer.4.attention.self.value.weight', 'generator.encoder.layer.0.output.dense.weight', 'generator.encoder.layer.5.output.LayerNorm.weight', 'generator.encoder.layer.6.intermediate.dense.weight', 'generator.encoder.layer.6.attention.output.dense.bias', 'generator.encoder.layer.6.output.LayerNorm.bias', 'generator.encoder.layer.0.attention.self.value.weight', 'generator.embeddings_project.bias', 'generator.encoder.layer.1.attention.self.value.bias', 'generator.encoder.layer.1.attention.self.key.weight', 'generator.encoder.layer.1.attention.self.query.bias', 'generator.encoder.layer.4.intermediate.dense.bias', 'generator.encoder.layer.9.output.LayerNorm.weight', 'generator.encoder.layer.4.output.dense.bias', 'generator.embeddings.position_embeddings.weight', 'generator.encoder.layer.8.intermediate.dense.bias', 'generator.encoder.layer.2.output.dense.bias', 'generator.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.encoder.layer.5.attention.output.LayerNorm.weight', 'generator.encoder.layer.7.attention.self.value.weight', 'generator.encoder.layer.2.attention.self.query.weight', 'generator.encoder.layer.5.attention.output.LayerNorm.bias', 'generator.encoder.layer.8.attention.self.query.bias', 'generator.encoder.layer.8.attention.self.key.bias', 'generator.encoder.layer.5.attention.self.key.bias', 'generator.encoder.layer.5.intermediate.dense.weight', 'generator.encoder.layer.10.attention.self.value.bias', 'generator.encoder.layer.0.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.key.weight', 'generator.encoder.layer.8.output.dense.bias', 'generator_predictions.dense.weight', 'generator.encoder.layer.2.attention.output.dense.weight', 'generator.encoder.layer.1.output.dense.bias', 'generator.encoder.layer.8.attention.self.query.weight', 'generator.encoder.layer.3.attention.self.query.bias', 'generator.encoder.layer.3.intermediate.dense.bias', 'generator.encoder.layer.4.attention.output.dense.bias', 'discriminator_predictions.LayerNorm.weight', 'generator.encoder.layer.1.output.LayerNorm.weight', 'generator.encoder.layer.0.attention.self.key.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.classifier.weight', 'generator.encoder.layer.2.attention.self.query.bias', 'generator.encoder.layer.6.attention.self.query.bias', 'generator.encoder.layer.0.output.LayerNorm.bias', 'generator.encoder.layer.4.output.LayerNorm.weight', 'generator.encoder.layer.11.attention.self.value.bias', 'generator.encoder.layer.6.attention.self.query.weight', 'generator.encoder.layer.8.output.LayerNorm.bias', 'generator.encoder.layer.1.output.dense.weight', 'generator.encoder.layer.2.intermediate.dense.weight', 'generator.encoder.layer.0.output.dense.bias', 'generator.encoder.layer.4.intermediate.dense.weight', 'generator.encoder.layer.8.output.dense.weight', 'generator.encoder.layer.3.attention.output.dense.weight', 'generator.encoder.layer.9.attention.self.value.bias', 'generator.encoder.layer.0.attention.output.LayerNorm.bias', 'generator.encoder.layer.3.attention.self.key.weight', 'generator_predictions.decoder.weight', 'generator.encoder.layer.11.attention.self.key.bias', 'generator.encoder.layer.7.attention.self.key.bias', 'generator.encoder.layer.0.attention.output.dense.bias', 'generator_predictions.LayerNorm.bias', 'generator.encoder.layer.6.attention.output.dense.weight', 'generator.encoder.layer.3.intermediate.dense.weight', 'generator.encoder.layer.9.attention.self.key.weight', 'generator.encoder.layer.0.attention.self.value.bias', 'generator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator_predictions.decoder.bias', 'generator.encoder.layer.7.attention.output.LayerNorm.weight', 'generator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.encoder.layer.2.attention.self.value.weight', 'generator.encoder.layer.9.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.output.dense.bias', 'generator.encoder.layer.11.output.LayerNorm.bias', 'generator.encoder.layer.11.attention.output.dense.bias', 'generator.encoder.layer.5.attention.output.dense.bias', 'generator.embeddings.token_type_embeddings.weight', 'generator.encoder.layer.10.attention.self.query.bias', 'generator.encoder.layer.5.output.LayerNorm.bias', 'generator.encoder.layer.11.output.dense.bias', 'generator.encoder.layer.7.attention.output.dense.weight', 'generator.encoder.layer.5.intermediate.dense.bias', 'generator.encoder.layer.8.output.LayerNorm.weight', 'generator.encoder.layer.10.attention.self.value.weight', 'generator.encoder.layer.0.attention.output.dense.weight', 'generator.embeddings.LayerNorm.weight', 'generator.encoder.layer.9.output.LayerNorm.bias', 'generator.encoder.layer.7.intermediate.dense.bias', 'generator.encoder.layer.1.output.LayerNorm.bias', 'generator.encoder.layer.1.intermediate.dense.weight', 'generator.encoder.layer.2.attention.self.value.bias', 'generator.encoder.layer.1.attention.self.query.weight', 'generator.encoder.layer.5.attention.self.value.bias', 'generator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.encoder.layer.9.attention.self.query.weight', 'generator.encoder.layer.7.attention.self.query.bias', 'generator.encoder.layer.6.attention.self.key.bias', 'generator.encoder.layer.3.output.dense.weight', 'generator.encoder.layer.4.attention.self.query.weight', 'generator.encoder.layer.4.attention.self.key.weight', 'generator.encoder.layer.11.intermediate.dense.bias', 'generator.encoder.layer.0.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.value.weight', 'generator.encoder.layer.2.output.dense.weight', 'generator.encoder.layer.11.attention.self.query.weight', 'generator.encoder.layer.10.attention.output.LayerNorm.weight', 'generator.encoder.layer.5.attention.self.query.bias', 'generator.encoder.layer.7.attention.self.query.weight', 'generator.encoder.layer.10.output.dense.bias', 'generator.encoder.layer.11.output.dense.weight', 'generator.encoder.layer.0.attention.self.query.bias', 'generator.encoder.layer.8.attention.output.dense.weight', 'generator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.encoder.layer.6.attention.output.LayerNorm.bias', 'generator.encoder.layer.6.attention.self.value.weight', 'generator.encoder.layer.10.output.dense.weight', 'generator.encoder.layer.10.intermediate.dense.weight', 'generator.encoder.layer.1.attention.output.dense.weight', 'generator.encoder.layer.0.attention.self.query.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading topic model\n",
    "MODEL_PATH = Path(\"../models/\")\n",
    "DATA_DIR = Path(\"../../BscThesisData/data\")\n",
    "topic_model = BERTopic.load(str(MODEL_PATH / \"topic_model\"), embedding_model=\"Maltehb/-l-ctra-danish-electra-small-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = read_pickle(DATA_DIR / \"paragraph_dict.pkl\")\n",
    "paragraph_list = get_paragraphs(clean_paragraphs)\n",
    "vectorizer = read_pickle(MODEL_PATH / \"vectorizer.pkl\")\n",
    "X = vectorizer.fit_transform(paragraph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sparse matrix into gensim corpus\n",
    "corpus_vect_gensim = Sparse2Corpus(X, documents_columns=False)\n",
    "dictionary = Dictionary.from_corpus(corpus_vect_gensim,\n",
    "                                    id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = vectorizer.build_tokenizer()\n",
    "tokenized_docs = [[word.lower() for word in tokenizer(doc) if not word.lower() in vectorizer.stop_words] for doc in paragraph_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Topics \n",
    "topics = topic_model.get_topics()\n",
    "topic_list = [[item[0] for item in topic] for key, topic in topics.items() if not key==-1]\n",
    "assert len(topic_list) == len(list(topics.keys())) - 1\n",
    "assert type(topic_list) == list\n",
    "assert type(topic_list[0]) == list\n",
    "assert type(topic_list[0][0]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(topics=topic_list, coherence=\"c_v\", dictionary=dictionary, texts=tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5138103769278116,\n",
       " 0.6713135994442715,\n",
       " 0.6199477050234451,\n",
       " 0.4090339291266286,\n",
       " 0.47907132277369946]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n",
      "superflous words: set()\n"
     ]
    }
   ],
   "source": [
    "all_words = set(word.lower() for doc in tokenized_docs for word in doc)\n",
    "topic_words = set(topic_list[0])\n",
    "\n",
    "for topic in topic_list:\n",
    "    topic_words = set(topic)\n",
    "    print(f\"superflous words: {topic_words - all_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3043144840452862, 0.27015012488366497, 0.4865898101777429, 0.6728787468629828, 0.41105431905355916, 0.416226571950254]\n",
      "0.4268690094955816\n"
     ]
    }
   ],
   "source": [
    "# Test if it works to remove invalid words :)) \n",
    "clean_topic_list = [[word for word in topic if word in all_words] for topic in topic_list]\n",
    "cm = CoherenceModel(topics=clean_topic_list, coherence=\"c_v\", dictionary=dictionary, texts=tokenized_docs)\n",
    "print(cm.get_coherence_per_topic())\n",
    "print(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f86e4359b2b5cf5d398b363c816bb50d8f4fd2c9e9291492e7c9219700f63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('bertopic_explore': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
