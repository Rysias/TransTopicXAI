{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitatitive topic evaluations\n",
    "This notebook is used for formatting the data for qualitative evaluations. This includes: \n",
    "- Inspecting the topic words\n",
    "- Inspecting representative documents\n",
    "- Coming up with good \"titles\" for the topics\n",
    "\n",
    "Notes on TweetEval: \n",
    "- 10 topics may be too much :((\n",
    "- Difficult to classify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pprint import pprint\n",
    "from typing import Dict, Tuple, List, Union\n",
    "from bertopic import BERTopic\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_model_name(model_path: Path) -> str:\n",
    "    return re.match(\"\\w+-\\w+-\\d+\", model_path.name).group()\n",
    "\n",
    "def create_topic_dict(raw_topic_dict: Dict[int, Tuple[str, int]]) -> Dict[int, List[str]]:\n",
    "    return {k: [tup[0] for tup in lst] for k, lst in raw_topic_dict.items() if k!=-1}\n",
    "\n",
    "def latest_full_topics(dr: Path) -> Path:\n",
    "    return list(dr.glob(\"*full_doc_topics_*.csv\"))[-1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "MODEL_DIR = Path(\"../../ExplainlpTwitter/output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "model = BERTopic.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_topics = pd.read_csv(latest_full_topics(DATA_DIR), index_col=0)\n",
    "doc_topics\n",
    "topic_words = read_pickle(DATA_DIR / \"tweeteval_topic_dict.pkl\" )\n",
    "clean_tweets = pd.read_csv(DATA_DIR / \"tweeteval_text.csv\", usecols=[\"text\"])\n",
    "doc_topics = doc_topics.merge(clean_tweets, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = create_topic_dict(topic_words)\n",
    "pprint(topic_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics.groupby(\"topic\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top, words in topic_dict.items():\n",
    "    print(f\"evaluating topic {top}\")\n",
    "    pprint(f\"{words = }\")\n",
    "    print(f\"examples for topic {top}\")\n",
    "    example_tweets = doc_topics.loc[doc_topics[\"topic\"] == top, \"text\"].sample(10, random_state=42).tolist()\n",
    "    print(example_tweets)\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6f86e4359b2b5cf5d398b363c816bb50d8f4fd2c9e9291492e7c9219700f63c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('bertopic_explore': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
